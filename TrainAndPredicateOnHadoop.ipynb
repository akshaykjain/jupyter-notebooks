{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Name is: rated3ProjectV30\n",
      "DSXHI systerm display name is : rated3\n",
      "USER ID is : 1000331002 , Project ID is ea65b3b4-173c-41f4-a6d0-3c048987c47d and project_name is rated3ProjectV30\n"
     ]
    }
   ],
   "source": [
    "# Setup prior to Test\n",
    "# find out what dsxhi register system to use by the project name.  Project is named with dsxhi register system name\n",
    "import json\n",
    "import os\n",
    "_project_name_= os.getenv(\"PROJECT_NAME\")\n",
    "if (not _project_name_) :\n",
    " print(\"No project name found, stop!!!\")\n",
    "else :\n",
    " print(\"Project Name is: \" + _project_name_ )\n",
    "\n",
    "_project_id_=os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "# When I created the project, I named the project prefixed with edge node short name\n",
    "edge_node_name =_project_name_.split('Project',1)\n",
    "\n",
    "dsxhi_register_name = edge_node_name[0]\n",
    "print(\"DSXHI systerm display name is : \" + dsxhi_register_name)\n",
    "_user_id_= os.getenv(\"USER_ID\")\n",
    "print(\"USER ID is : \" + _user_id_ + \" , Project ID is \" + _project_id_ + \" and project_name is \" + _project_name_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for livy version LIVYSPARK2 in dsxhi registered display name rated3 and Image_id name jupyter-py36\n",
      "\n",
      "\n",
      "Available Hadoop systems: \n",
      "\n",
      "      systemName LIVYSPARK  LIVYSPARK2       imageId\n",
      "0   cdh-outsider            livyspark2  jupyter-py36\n",
      "1  cdhv62-edge-1            livyspark2  jupyter-py36\n",
      "2     datapuddle            livyspark2              \n",
      "3     hdp31edge1            livyspark2              \n",
      "4         rated3            livyspark2  jupyter-py36\n",
      "5         yccdh5            livyspark2              \n",
      "Skip this system cdh-outsider\n",
      "Skip this system cdhv62-edge-1\n",
      "Skip this system datapuddle\n",
      "Skip this system hdp31edge1\n",
      "my_livy_endpoint in system rated3 livy endpoint found is: https://internal-nginx-svc:12443/ibm-dsxhi-rated3/livy2/v1\n",
      "\n",
      "\n",
      "DSXHI register system: rated3\n",
      "Python image ID: jupyter-py36\n",
      "Livy endpoint: https://internal-nginx-svc:12443/ibm-dsxhi-rated3/livy2/v1\n",
      "Webhdfs endpoint: https://internal-nginx-svc:12443/ibm-dsxhi-rated3/webhdfs/v1\n"
     ]
    }
   ],
   "source": [
    "# Setup prior to Test\n",
    "import hadoop_lib_utils\n",
    "my_image_id = \"\"\n",
    "image_list = \"\"\n",
    "my_Livy_endpoint = \"\"\n",
    "found = 0\n",
    "# Looking for this image ID\n",
    "python_image_id =\"jupyter-py36\"\n",
    "\n",
    "# Looking for livy version\n",
    "livy_spark = \"LIVYSPARK2\"\n",
    "print(\"Looking for livy version \" + livy_spark + \" in dsxhi registered display name \" + dsxhi_register_name \n",
    "      + \" and Image_id name \" + python_image_id + \"\\n\\n\")\n",
    "\n",
    "dsxhi_reg_info = hadoop_lib_utils.get_dsxhi_info(showSummary=True)\n",
    "\n",
    "webhdfs_name = \"WEBHDFS\"\n",
    "my_webhdfs_endpoint = \"\"\n",
    "webhdfs_found = 0\n",
    "for row in dsxhi_reg_info :\n",
    "    # print(json.dumps(row))\n",
    "    \n",
    "    system = row[\"system\"]\n",
    "    \n",
    "    if (system != dsxhi_register_name):\n",
    "        print(\"Skip this system \" + system)\n",
    "        continue\n",
    "        \n",
    "    my_livy_endpoint = row[livy_spark]\n",
    "    my_webhdfs_endpoint = row[webhdfs_name]\n",
    "\n",
    "    if (my_livy_endpoint):\n",
    "        # print(\"Found dsxhi systerm with livy spark url\" + my_livy_endpoint)\n",
    "        # print(\"Found dsxhi systerm with webhdfs url\" + my_webhdfs_endpoint)\n",
    "        print(\"my_livy_endpoint in system \" + system + \" livy endpoint found is: \" + my_livy_endpoint)\n",
    "         \n",
    "        image_list = row[\"imageList\"]\n",
    "                       \n",
    "        for image_id in image_list:\n",
    "            # print (\"Image ID is \" + image_id)\n",
    "            if (image_id.find(python_image_id) >= 0):\n",
    "                my_image_id = python_image_id\n",
    "                # print (\"Found the image ID \" + image_id)\n",
    "                found = 1\n",
    "                break                   \n",
    "       \n",
    "    if (found):\n",
    "        break\n",
    "\n",
    "print(\"\\n\\nDSXHI register system: \" + dsxhi_register_name)                \n",
    "print(\"Python image ID: \" + my_image_id)   \n",
    "print(\"Livy endpoint: \" +  my_livy_endpoint) \n",
    "print(\"Webhdfs endpoint: \" + my_webhdfs_endpoint) \n",
    "if (not my_livy_endpoint or not my_webhdfs_endpoint):\n",
    "   print(\"!!!STOP!!!, require info not found !!!!\")\n",
    "if (not found):\n",
    "    print(\"Image ID not found !!!!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to run Livy sessions on a remote Hadoop cluster, your Watson Studio admin must first \n",
    "### register a Hadoop Integration system with Watson Studio.\n",
    "\n",
    "### Ask your Watson Studio admin to use the Admin Console => Hadoop Integration option to register \n",
    "### a Hadoop Integration system. NOTE: Installation and configuration of IBM's Hadoop Integration (HI) \n",
    "### service on a Hadoop cluster must be done by a Hadoop admin before that system can be registered with your Watson Studio account.\n",
    "\n",
    "### When your admin indicates that a Hadoop Integration system has been registered, you can proceed with this sample notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.6-WMLCE/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Imports needed for the cells which run locally on Watson Studio.\n",
    "# import dsx_core_utils\n",
    "import hadoop_lib_utils\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the Spark session that we will run on the selected registered HI system. \n",
    "#### In this case we want the session to start with 1G memory and two Spark executors. \n",
    "### NOTE: myConfig here is optional; if you prefer to use default configs you can omit this cell and remove the addlConfig argument in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "myConfig={\n",
    " \"queue\": \"default\",\n",
    " \"driverMemory\": \"1G\",\n",
    " \"numExecutors\": 2\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSXHI registered name is rated3 Spark version is livyspark2 image ID is jupyter-py35\n",
      "sparkmagic has been configured to use https://ibm-dsxhi-rated3-svc:8443/livy2/v1 \n",
      "success configuring sparkmagic livy.\n"
     ]
    }
   ],
   "source": [
    "# Set up sparkmagic to connect to the selected registered HI\n",
    "# system with the specified configs. **NOTE** This notebook\n",
    "# requires Spark 2, so you should set 'livy' to 'livyspark2'.\n",
    "# Use imageId=\"jupyter\" if running the notebook on Python 3.5 \n",
    "\n",
    "my_imageID = \"jupyter-py35\"\n",
    "livy_version = \"livyspark2\"\n",
    "print(\"DSXHI registered name is \" + dsxhi_register_name + \" Spark version is \" + livy_version + \" image ID is \" + my_imageID)\n",
    "HI_CONFIG = hadoop_lib_utils.setup_livy_sparkmagic(\n",
    "  system=dsxhi_register_name, \n",
    "  livy=livy_version,\n",
    "  # imageId=my_imageID,\n",
    "  addlConfig=myConfig)\n",
    "\n",
    "# (Re-)load sparkmagic to apply the new configs.\n",
    "%reload_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's capture some state about the configured Hadoop Integraton system, to be used later in this notebook. \n",
    "### Then start up a new, remote Livy session to connect to that HI system. NOTE: Depending on \n",
    "### a) the resources available in the remote Hadoop system and \n",
    "### b) the speed of your cluster, attempts to start the session might report errors due to timeout or due to a session coming up dead. \n",
    "### In such cases you should run %spark cleanup as a separate cell, then re-run this cell again. \n",
    "### If session creation continues to fail, contact the Hadop admin of the target Hadoop cluster to see if everything is configured as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2045</td><td>application_1570832601630_0125</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://rated1.fyre.ibm.com:8088/proxy/application_1570832601630_0125/\">Link</a></td><td><a target=\"_blank\" href=\"http://rated6.fyre.ibm.com:8042/node/containerlogs/container_e29_1570832601630_0125_01_000001/user1\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "session_name = 'mlsess1'\n",
    "livy_endpoint = HI_CONFIG['LIVY']\n",
    "webhdfs_endpoint = HI_CONFIG['WEBHDFS']\n",
    "%spark add -s $session_name -l python -k -u $livy_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For reference / debugging: Print out the name of the Hadoop node to which the remote session has been assigned. \n",
    "### When \"local\" files are created within the remote session, they will be written to this node. \n",
    "### All of the Yarn container artifacts (workspace and temp files) will exist on this node, as well.\n",
    "### Test spark job by getting catalog info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remote livy session driver: rated6.fyre.ibm.com"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "import socket\n",
    "print(\"Remote livy session driver: {}\".format(socket.gethostname()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cell, and all subsequent cells which have %%spark as their first line, will run remotely, \n",
    "### i.e. within a Yarn container that exists on the registered Hadoop Integration system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "\n",
    "# Declare imports needed for all of the cells that will run remotely.\n",
    "import getpass, time, os, shutil\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Load IBM Hadoop Integration utilities to facilitate remote functionality.\n",
    "# This line assumes that HI version >= X.Y has been installed on the registered\n",
    "# Hadoop Integration system.\n",
    "hi_utils_lib = os.getenv(\"HI_UTILS_PATH\", \"\")\n",
    "sc.addPyFile(\"hdfs://{}\".format(hi_utils_lib))\n",
    "import hi_core_utils\n",
    "\n",
    "# Declare a target HDFS directory path that will be used for our data.\n",
    "hdfs_dataset_dir = \"/user/{}/datasets\".format(getpass.getuser())\n",
    "input_ds = \"{}/{}\".format(hdfs_dataset_dir, \"cars.csv\")\n",
    "\n",
    "# Create target hdfs directory, if it does not already exist.\n",
    "# I have this dir in my hadoop already, skip\n",
    "# hi_core_utils.run_command(\"hdfs dfs -mkdir -p {}\".format(hdfs_dataset_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's load our test data into HDFS. For the purposes of this sample, our data is small and comes from the local cars.csv file created above. \n",
    "### We do not need to put it into HDFS for this example--but we choose to do so for demonstration purposes. \n",
    "###  In a real scenario the desired data should already be loaded into HDFS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redeclare the dataset dir locally--the earlier declaration was in the _remote_\n",
    "# session so it is not available here.\n",
    "\n",
    "# ** NOTE ** Replace {your-username-here} with your actual user name.\n",
    "# *** Skip the following because I have the setup in my hadoop system *** \n",
    "# hdfs_dataset_dir = \"/user/user1/datasets\"\n",
    "\n",
    "# Upload the saved csv file from Local to the remote HDFS.\n",
    "#input_csv = os.environ[\"DSX_PROJECT_DIR\"] + \"/datasets/cars.csv\"\n",
    "# input_csv = \"/project_data/data_asset/cars.csv\"\n",
    "# hadoop_lib_utils.upload_hdfs_file(webhdfs_endpoint, input_csv, \"{}/cars.csv\".format(hdfs_dataset_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access and Manipulate Data\n",
    "### Now use Spark to read the data, as a Spark dataframe, from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+----------+------+------------+----+--------+--------------------+\n",
      "|mpg|cylinders|engine|horsepower|weight|acceleration|year|  origin|                name|\n",
      "+---+---------+------+----------+------+------------+----+--------+--------------------+\n",
      "| 18|        8| 307.0|       130|  3504|        12.0|  70|American|chevrolet chevell...|\n",
      "| 15|        8| 350.0|       165|  3693|        11.5|  70|American|   buick skylark 320|\n",
      "| 18|        8| 318.0|       150|  3436|        11.0|  70|American|  plymouth satellite|\n",
      "| 16|        8| 304.0|       150|  3433|        12.0|  70|American|       amc rebel sst|\n",
      "| 17|        8| 302.0|       140|  3449|        10.5|  70|American|         ford torino|\n",
      "+---+---------+------+----------+------+------------+----+--------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "\n",
    "df_data_0 = spark.read.format(\n",
    "    \"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").option(\n",
    "    \"header\", \"true\").option(\"inferSchema\", \"true\").load(input_ds)\n",
    "\n",
    "df_data_0.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due to missing data in the mpg and horsepower columns, they will be excluded from the dataset for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------------+----+--------+--------------------+\n",
      "|cylinders|engine|weight|acceleration|year|  origin|                name|\n",
      "+---------+------+------+------------+----+--------+--------------------+\n",
      "|        8| 307.0|  3504|        12.0|  70|American|chevrolet chevell...|\n",
      "|        8| 350.0|  3693|        11.5|  70|American|   buick skylark 320|\n",
      "|        8| 318.0|  3436|        11.0|  70|American|  plymouth satellite|\n",
      "|        8| 304.0|  3433|        12.0|  70|American|       amc rebel sst|\n",
      "|        8| 302.0|  3449|        10.5|  70|American|         ford torino|\n",
      "+---------+------+------+------------+----+--------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "carsDataRaw = df_data_0\n",
    "carsModData = carsDataRaw.drop(\"mpg\").drop(\"horsepower\")\n",
    "carsModData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the model training process, the original dataset will be split into a training dataset and a testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training dataset: 348\n",
      "Number of testing dataset: 58"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "\n",
    "splitted_data = carsModData.randomSplit([0.85, 0.15], 24)\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "\n",
    "print(\"Number of training dataset: {}\".format(train_data.count()))\n",
    "print(\"Number of testing dataset: {}\".format(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now set the input columns for model training, and use the corresponding algorithms to train the model.\n",
    "### In this example, the Linear Regression method is used to evaluate weight in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "originIndexer = StringIndexer().setInputCol(\"origin\").setOutputCol(\"origin_code\")\n",
    "vectorAssembler_features = VectorAssembler().setInputCols(\n",
    "    [\"cylinders\", \"engine\", \"acceleration\", \"year\", \"origin_code\"]).setOutputCol(\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "rf = LinearRegression().setLabelCol(\"weight\").setFeaturesCol(\"features\")\n",
    "pipeline = Pipeline().setStages([originIndexer,vectorAssembler_features,rf])\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "### The model performance can be evaluated using the R Square for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Square of Test Data: 0.863976844308"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "testData = model.transform(test_data).drop(\"prediction\")\n",
    "metric = model.stages[2].evaluate(testData)\n",
    "print(\"R Square of Test Data: {}\".format(metric.r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the model to HDFS\n",
    "### First, in the remote session, we use a Hadoop Integration utility method to write the model to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': 'hdfs:///user/user1/.dsxhi/models/ml_cars_model_pn_1/20/model', 'version': 20, 'name': 'ml_cars_model_pn_1', 'latest_version': 20}\n",
      "Using TensorFlow backend."
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "hi_core_utils.write_model_to_hdfs(model=model, model_name=\"ml_cars_model_pn_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the model from HDFS to WS-Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download success\n"
     ]
    }
   ],
   "source": [
    "# Redeclare the dataset dir locally--the earlier declaration was in the _remote_\n",
    "# session so it is not available here.\n",
    "\n",
    "# ** NOTE ** Replace {your-username-here} with your actual user name.\n",
    "hdfs_dataset_dir = \"/user/user1/.dsxhi/models/ml_cars_model_pn_1/1/model.tar.gz\"\n",
    "\n",
    "# Download the model from HDFS to WS-Local.\n",
    "#input_csv = os.environ[\"DSX_PROJECT_DIR\"] + \"/datasets/cars.csv\"\n",
    "#input_csv = \"/project_data/data_asset/cars.csv\"\n",
    "wsl_model_path = \"/project_data/data_asset/model.tar.gz\"\n",
    "hadoop_lib_utils.download_hdfs_file(webhdfs_endpoint, hdfs_dataset_dir, wsl_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next use watson_machine_learning_client to process the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 + WML CE 1.6.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
